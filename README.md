# A SMART Mnemonic Sounds like ‚ÄúGlue Tonic‚Äù: Mixing LLMs with Student Feedback to Make Mnemonic Learning Stick

This repository contains the code, data, and pre-trained models for our Arxiv paper: **A SMART Mnemonic Sounds like ‚ÄúGlue Tonic‚Äù: Mixing LLMs with Student Feedback to Make Mnemonic Learning Stick**

<h3 align="center">
<span style="color:black">ü¶æ <a style="color:black;" href="https://huggingface.co/collections/nbalepur/mnemonic-generation-6674c357b3882fd58790ebd4">Model</a>&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;üìä <a href="https://huggingface.co/collections/nbalepur/mnemonic-generation-6674c357b3882fd58790ebd4">Data</a>&nbsp;&nbsp;&nbsp;  &nbsp;&nbsp;&nbsp;üìù <a href="https://www.overleaf.com/read/hykpqxvmzgbt#5b4ca1">Paper</a></span>
</h3>

<br />

![Mnemonic_Figure](https://github.com/nbalepur/Mnemonic/assets/55101514/de8fd5be-2a02-4d0c-a170-5e56138f3ab8)

## Abstract

Keyword mnemonics are memorable explanations that link new terms to simpler keywords.
Prior works generate mnemonics for students, but they do not guide models toward mnemonics students prefer and aid learning.
We build SMART, a mnemonic generator trained on feedback from real students learning new terms.
To train SMART, we first fine-tune LLaMA-2 on a~curated set of user-written mnemonics.
We then use LLM alignment to enhance \model: we deploy mnemonics generated by SMART in a flashcard app to find preferences on mnemonics students favor.
We gather 2684 preferences from 45 students across two types: **expressed** (inferred from ratings) and **observed** (inferred from student learning), yielding three key findings:

1. Expressed and observed preferences disagree; what students *think* is helpful does not fully capture what is *truly* helpful
2. Bayesian models can synthesize complementary data from multiple preference types into a single effectiveness signal.
SMART is tuned via Direct Preference Optimization on this signal, which we show resolves ties and missing labels in the typical method of pairwise comparisons, augmenting data for LLM output quality gains. 
3. Mnemonic experts assess SMART as matching GPT-4, at much lower deployment costs, showing the utility of capturing diverse student feedback to align LLMs in education.

## Mnemonic Datasets
- Mnemonic Fine-tuning Data: https://huggingface.co/datasets/nbalepur/Mnemonic_SFT
- Mnemonic Student Preferences Data: https://huggingface.co/datasets/nbalepur/Mnemonic_Pref
- Mnemonic Test Set: https://huggingface.co/datasets/nbalepur/Mnemonic_Test

## Pre-trained SMART Models
- SMART Tokenizer: https://huggingface.co/datasets/nbalepur/LLama-2-70b-Mnemonic-Tokenizer
- SMART Fine-tuned: https://huggingface.co/nbalepur/LLama-2-70b-Mnemonic-SFT
- SMART DPO: https://huggingface.co/nbalepur/LLama-2-70b-Mnemonic-DPO/
